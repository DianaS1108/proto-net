{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_list_to_file(list, path):\n",
    "    with open(path, \"w\") as f:\n",
    "        for item in list:\n",
    "            f.write(f\"{item}\\n\")\n",
    "\n",
    "def read_file_to_list(path):\n",
    "    with open(path, \"r\") as f:\n",
    "        list = f.read().splitlines()\n",
    "        list = [float(item) for item in list]\n",
    "    return list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.join(\".\", \"data_cub\")  # change as need\n",
    "\n",
    "num_way_tr = 50\n",
    "num_query_tr = 10\n",
    "\n",
    "num_way_val = 50\n",
    "num_query_val = 10\n",
    "\n",
    "# training\n",
    "num_iter = 100\n",
    "\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results?\n",
    "\n",
    "save = False\n",
    "\n",
    "if save == True:\n",
    "    output_dir = os.path.join(\".\", \"output\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attributes():\n",
    "    with open(os.path.join(data_dir, \"class_attribute_labels_continuous.txt\")) as f:\n",
    "        attributes = f.read().splitlines()\n",
    "        attributes = [a.split(\" \") for a in attributes]\n",
    "        attributes = [[float(a) for a in aa] for aa in attributes]\n",
    "        attributes = torch.Tensor(attributes)\n",
    "    return attributes\n",
    "\n",
    "attributes = get_attributes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classes(path):\n",
    "    with open(path) as f:\n",
    "        classes = f.read().splitlines()\n",
    "    return classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(mode, classes):\n",
    "    x_list = []\n",
    "    v_list = []\n",
    "    y_list = []\n",
    "    for c in tqdm(classes, desc=\"Class\"):\n",
    "        y = int(c.split(\".\")[0])\n",
    "        v = attributes[y - 1]\n",
    "\n",
    "        xs = torch.Tensor(torch.load(os.path.join(data_dir, \"images\", c)))\n",
    "        xs = torch.swapaxes(xs, 1, 2)\n",
    "        for i in range(xs.shape[0]):\n",
    "            if mode == \"train\":\n",
    "                for j in range(xs.shape[1]):\n",
    "                    x = xs[i][j]\n",
    "                    x_list.append(x)\n",
    "                    v_list.append(v)\n",
    "                    y_list.append(y)\n",
    "            else:\n",
    "                x = xs[i][0]\n",
    "                x_list.append(x)\n",
    "                v_list.append(v)\n",
    "                y_list.append(y)\n",
    "    \n",
    "    return x_list, v_list, y_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CUBDataset(Dataset):\n",
    "    def __init__(self, mode):\n",
    "        self.classes = get_classes(os.path.join(data_dir, mode + \"classes.txt\"))\n",
    "\n",
    "        self.x, self.v, self.y = get_data(mode, self.classes)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.v[idx], self.y[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CUBDataset(\"train\")\n",
    "val_dataset = CUBDataset(\"val\")\n",
    "test_dataset = CUBDataset('test')\n",
    "trainval_dataset = CUBDataset(\"trainval\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_indices(labels, classes):\n",
    "    dic = {}\n",
    "    for c in tqdm(classes, desc=\"Class\"):\n",
    "        dic[c] = np.where(labels == c)[0]\n",
    "    return dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrototypicalBatchSampler():\n",
    "    def __init__(self, labels, num_way, num_samples, num_iter):\n",
    "        super().__init__()\n",
    "        self.num_way = num_way\n",
    "        self.num_samples = num_samples\n",
    "        self.num_iter = num_iter\n",
    "\n",
    "        self.classes = np.unique(labels)\n",
    "        self.class_indices = get_class_indices(labels, self.classes)\n",
    "\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for it in range(self.num_iter):\n",
    "            batch = np.empty(self.num_way * self.num_samples, dtype=np.int64)\n",
    "            \n",
    "            # select classes\n",
    "            c_idxs = torch.randperm(len(self.classes))[:self.num_way]\n",
    "\n",
    "            # select samples\n",
    "            for i, c in enumerate(self.classes[c_idxs]):\n",
    "                s_idxs = torch.randperm(len(self.class_indices[c]))[:self.num_samples]\n",
    "\n",
    "                sl = slice(i * self.num_samples, (i + 1) * self.num_samples)\n",
    "                batch[sl] = self.class_indices[c][s_idxs]\n",
    "                \n",
    "            yield batch\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_iter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def get_dataloader(dataset, num_way, num_query, num_iter):\n",
    "    sampler = PrototypicalBatchSampler(dataset.y, num_way, num_query, num_iter)\n",
    "    return DataLoader(dataset, batch_sampler=sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = get_dataloader(train_dataset, num_way_tr, num_query_tr, num_iter)\n",
    "val_dataloader = get_dataloader(val_dataset, num_way_val, num_query_val, num_iter)\n",
    "test_dataloader = get_dataloader(test_dataset, num_way_val, num_query_val, num_iter)\n",
    "trainval_dataloader = get_dataloader(trainval_dataset, num_way_val, num_query_val, num_iter)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn.functional import normalize\n",
    "\n",
    "class ProtoNet(nn.Module):\n",
    "    def __init__(self, x_dim = 1024, v_dim = 312, emb_dim = 1024):\n",
    "        super(ProtoNet, self).__init__()\n",
    "        \n",
    "        self.x_encoder = nn.Linear(x_dim, emb_dim)\n",
    "        self.v_encoder = nn.Linear(v_dim, emb_dim)\n",
    "    \n",
    "    def forward(self, x, v):\n",
    "        x = self.x_encoder(x)\n",
    "        v = self.v_encoder(v)\n",
    "        v = normalize(v)\n",
    "        return x, v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ProtoNet().to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_dist_matrix(x, y):\n",
    "    # x: n x d\n",
    "    # y: m x d\n",
    "    n = x.shape[0]\n",
    "    m = y.shape[0]\n",
    "    d = x.shape[1]\n",
    "\n",
    "    x = x.unsqueeze(1).expand(n, m, d)\n",
    "    y = y.unsqueeze(0).expand(n, m, d)\n",
    "\n",
    "    return torch.pow(x - y, 2).sum(2)  # n x m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import log_softmax\n",
    "\n",
    "def compute_loss_acc(x, v, y):\n",
    "    classes = torch.unique(y)\n",
    "\n",
    "    target_idxs = torch.Tensor([]).to(device)\n",
    "    for i, c in enumerate(classes):\n",
    "        c_idxs = torch.where(y == c)[0]\n",
    "        \n",
    "        c_prototype = v[c_idxs][0]\n",
    "        c_query = x[c_idxs]\n",
    "        c_target_idxs = torch.ones(len(c_query), dtype=torch.int64) * i\n",
    "        c_target_idxs = c_target_idxs.to(device)\n",
    "\n",
    "        if i == 0:\n",
    "            prototypes = c_prototype\n",
    "            query = c_query\n",
    "            target_idxs = c_target_idxs\n",
    "        else:\n",
    "            prototypes = torch.vstack((prototypes, c_prototype))\n",
    "            query = torch.vstack((query, c_query))\n",
    "            target_idxs = torch.hstack((target_idxs, c_target_idxs))\n",
    "        \n",
    "    dists = compute_dist_matrix(prototypes, query)\n",
    "    log_prob = log_softmax(-dists, dim=0)\n",
    "\n",
    "    target_matrix = torch.zeros_like(log_prob)\n",
    "    target_matrix[(target_idxs, torch.arange(len(query)))] = torch.ones(len(query)).to(device)\n",
    "\n",
    "    loss = (-log_prob * target_matrix).mean()\n",
    "\n",
    "    pred = torch.max(log_prob, dim=0).indices\n",
    "    acc = (target_idxs == pred).float().mean()\n",
    "\n",
    "    return loss, acc\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "lr = 1e-4\n",
    "weight_decay = 1e-5\n",
    "\n",
    "optimiser = Adam(model.parameters(), lr=lr, weight_decay=weight_decay)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = {\n",
    "    \"total_epoch\": 0,\n",
    "    \"train_loss\": [],\n",
    "    \"train_acc\": [],\n",
    "    \"val_loss\": [],\n",
    "    \"val_acc\": [],\n",
    "    \"best_acc\": 0,\n",
    "    \"best_epoch\": None,\n",
    "    \"best_model\": None\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    global history\n",
    "\n",
    "    for ep in range(epoch):\n",
    "        history[\"total_epoch\"] += 1\n",
    "\n",
    "        # train\n",
    "        model.train()\n",
    "        sum_loss = 0\n",
    "        sum_acc = 0\n",
    "        for x, v, y in train_dataloader:\n",
    "            x, v, y = x.to(device), v.to(device), y.to(device)\n",
    "            x_emb, v_emb = model(x, v)\n",
    "            loss, acc = compute_loss_acc(x_emb, v_emb, y)\n",
    "\n",
    "            optimiser.zero_grad()\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "\n",
    "            sum_loss += loss.item()\n",
    "            sum_acc += acc.item()\n",
    "        \n",
    "        avg_loss = sum_loss / num_iter\n",
    "        avg_acc = sum_acc / num_iter\n",
    "        history[\"train_loss\"].append(avg_loss)\n",
    "        history[\"train_acc\"].append(avg_acc)\n",
    "\n",
    "        # validation\n",
    "        model.eval()\n",
    "        sum_loss = 0\n",
    "        sum_acc = 0\n",
    "        for x, v, y in val_dataloader:\n",
    "            x, v, y = x.to(device), v.to(device), y.to(device)\n",
    "            x_emb, v_emb = model(x, v)\n",
    "            loss, acc = compute_loss_acc(x_emb, v_emb, y)\n",
    "\n",
    "            sum_loss += loss.item()\n",
    "            sum_acc += acc.item()\n",
    "        \n",
    "        avg_loss = sum_loss / num_iter\n",
    "        avg_acc = sum_acc / num_iter\n",
    "        history[\"val_loss\"].append(avg_loss)\n",
    "        history[\"val_acc\"].append(avg_acc)\n",
    "\n",
    "        if avg_acc > history[\"best_acc\"]:\n",
    "            history[\"best_acc\"] = avg_acc\n",
    "            history[\"best_epoch\"] = history[\"total_epoch\"]\n",
    "            history[\"best_model\"] = model.state_dict()\n",
    "        \n",
    "        print(f\"Epoch {history['total_epoch']}: Train Loss {history['train_loss'][-1]}, Acc {history['train_acc'][-1]}; Val Loss {history['val_loss'][-1]}, Acc {history['val_acc'][-1]}; Best {history['best_acc']} (Epoch {history['best_epoch']})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curve\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 4))\n",
    "\n",
    "ax1.plot(history[\"train_loss\"], label='train')\n",
    "ax1.plot(history[\"val_loss\"], label='val')\n",
    "ax1.grid()\n",
    "ax1.legend()\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_title('Loss')\n",
    "\n",
    "ax2.plot(history[\"train_acc\"], label='train')\n",
    "ax2.plot(history[\"val_acc\"], label='val')\n",
    "ax2.grid()\n",
    "ax2.legend()\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_title('Acc')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ProtoNet().to(device)\n",
    "model.load_state_dict(history['best_model'])\n",
    "\n",
    "optimiser = Adam(model.parameters(), lr=lr, weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrain_history = {\n",
    "    \"total_epoch\": 0,\n",
    "    \"train_loss\": [],\n",
    "    \"train_acc\": [],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrain(epoch):\n",
    "    global retrain_history\n",
    "\n",
    "    for ep in range(epoch):\n",
    "        retrain_history[\"total_epoch\"] += 1\n",
    "\n",
    "        # train\n",
    "        model.train()\n",
    "        sum_loss = 0\n",
    "        sum_acc = 0\n",
    "        for x, v, y in trainval_dataloader:\n",
    "            x, v, y = x.to(device), v.to(device), y.to(device)\n",
    "            x_emb, v_emb = model(x, v)\n",
    "            loss, acc = compute_loss_acc(x_emb, v_emb, y)\n",
    "\n",
    "            optimiser.zero_grad()\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "\n",
    "            sum_loss += loss.item()\n",
    "            sum_acc += acc.item()\n",
    "        \n",
    "        avg_loss = sum_loss / num_iter\n",
    "        avg_acc = sum_acc / num_iter\n",
    "        retrain_history[\"train_loss\"].append(avg_loss)\n",
    "        retrain_history[\"train_acc\"].append(avg_acc)\n",
    "        \n",
    "        print(f\"Epoch {retrain_history['total_epoch']}: Train Loss {retrain_history['train_loss'][-1]}, Acc {retrain_history['train_acc'][-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrain(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot retraining curve\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 4))\n",
    "\n",
    "ax1.plot(retrain_history[\"train_loss\"], label='trainval')\n",
    "ax1.grid()\n",
    "ax1.legend()\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_title('Loss')\n",
    "\n",
    "ax2.plot(retrain_history[\"train_acc\"], label='trainval')\n",
    "ax2.grid()\n",
    "ax2.legend()\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_title('Acc')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, epoch):\n",
    "    avg_acc = []\n",
    "    model.eval()\n",
    "    for ep in tqdm(range(epoch)):\n",
    "        for x, v, y in test_dataloader:\n",
    "            x, v, y = x.to(device), v.to(device), y.to(device)\n",
    "            x_emb, v_emb = model(x, v)\n",
    "            _, acc = compute_loss_acc(x_emb, v_emb, y)\n",
    "            avg_acc.append(acc.item())\n",
    "    avg_acc = np.mean(avg_acc)\n",
    "    print(f\"Test Acc: {avg_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(model, 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prototypical",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
